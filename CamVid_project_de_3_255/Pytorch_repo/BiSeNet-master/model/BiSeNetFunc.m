function [output, state] = BiSeNetFunc(input, params, varargin)
%BISENETFUNC Function implementing an imported ONNX network.
%
% THIS FILE WAS AUTO-GENERATED BY importONNXFunction.
% ONNX Operator Set Version: 11
%
% Variable names in this function are taken from the original ONNX file.
%
% [OUTPUT] = BiSeNetFunc(INPUT, PARAMS)
%			- Evaluates the imported ONNX network BISENETFUNC with input(s)
%			INPUT and the imported network parameters in PARAMS. Returns
%			network output(s) in OUTPUT.
%
% [OUTPUT, STATE] = BiSeNetFunc(INPUT, PARAMS)
%			- Additionally returns state variables in STATE. When training,
%			use this form and set TRAINING to true.
%
% [__] = BiSeNetFunc(INPUT, PARAMS, 'NAME1', VAL1, 'NAME2', VAL2, ...)
%			- Specifies additional name-value pairs described below:
%
% 'Training'
% 			Boolean indicating whether the network is being evaluated for
%			prediction or training. If TRAINING is true, state variables
%			will be updated.
%
% 'InputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			 between the dimensions of the input data and the dimensions of
%			the ONNX model input. For example, the permutation from HWCN
%			(MATLAB standard) to NCHW (ONNX standard) uses the vector
%			[4 3 1 2]. See the documentation for IMPORTONNXFUNCTION for
%			more information about automatic permutation.
%
%			'none' - Input(s) are passed in the ONNX model format. See 'Inputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between input data dimensions and the expected
%			ONNX input dimensions.%
%			cell array - If the network has multiple inputs, each cell
%			contains 'auto', 'none', or a numeric vector.
%
% 'OutputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			between the dimensions of the output and a conventional MATLAB
%			dimension ordering. For example, the permutation from NC (ONNX
%			standard) to CN (MATLAB standard) uses the vector [2 1]. See
%			the documentation for IMPORTONNXFUNCTION for more information
%			about automatic permutation.
%
%			'none' - Return output(s) as given by the ONNX model. See 'Outputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between the ONNX output dimensions and the
%			desired output dimensions.%
%			cell array - If the network has multiple outputs, each cell
%			contains 'auto', 'none' or a numeric vector.
%
% Inputs:
% -------
% INPUT
%			- Input(s) to the ONNX network.
%			  The input size(s) expected by the ONNX file are:
%				  INPUT:		[batch_size, 3, 720, 960]				Type: FLOAT
%			  By default, the function will try to permute the input(s)
%			  into this dimension ordering. If the default is incorrect,
%			  use the 'InputDataPermutation' argument to control the
%			  permutation.
%
%
% PARAMS	- Network parameters returned by 'importONNXFunction'.
%
%
% Outputs:
% --------
% OUTPUT
%			- Output(s) of the ONNX network.
%			  Without permutation, the size(s) of the outputs are:
%				  OUTPUT:		[batch_size, 12, 720, 960]				Type: FLOAT
%			  By default, the function will try to permute the output(s)
%			  from this dimension ordering into a conventional MATLAB
%			  ordering. If the default is incorrect, use the
%			  'OutputDataPermutation' argument to control the permutation.
%
% STATE		- (Optional) State variables. When TRAINING is true, these will
% 			  have been updated from the original values in PARAMS.State.
%
%
%  See also importONNXFunction

% Preprocess the input data and arguments:
[input, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(input, params, varargin{:});
% Put all variables into a single struct to implement dynamic scoping:
[Vars, NumDims] = packageVariables(params, {'input'}, {input}, [4]);
% Call the top-level graph function:
[output, outputNumDims, state] = main_graphGraph1000(input, NumDims.input, Vars, NumDims, Training, params.State);
% Postprocess the output data
[output] = postprocessOutput(output, outputDataPerms, anyDlarrayInputs, Training, varargin{:});
end

function [output, outputNumDims1098, state] = main_graphGraph1000(input, inputNumDims1097, Vars, NumDims, Training, state)
% Function implementing the graph 'main_graphGraph1000'
% Update Vars and NumDims from the graph's formal input parameters. Note that state variables are already in Vars.
Vars.input = input;
NumDims.input = inputNumDims1097;

% Execute the operators:
% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_saptial_path_convblock1_conv1_Conv_out] = prepareConvArgs(Vars.onnx__Conv_307, Vars.onnx__Conv_308, Vars.ConvStride1001, Vars.ConvDilationFactor1002, Vars.ConvPadding1003, 1, NumDims.input, NumDims.onnx__Conv_307);
Vars.x_saptial_path_convblock1_conv1_Conv_out = dlconv(Vars.input, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_saptial_path_convblock1_relu_Relu_outp = relu(Vars.x_saptial_path_convblock1_conv1_Conv_out);
NumDims.x_saptial_path_convblock1_relu_Relu_outp = NumDims.x_saptial_path_convblock1_conv1_Conv_out;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_saptial_path_convblock2_conv1_Conv_out] = prepareConvArgs(Vars.onnx__Conv_310, Vars.onnx__Conv_311, Vars.ConvStride1004, Vars.ConvDilationFactor1005, Vars.ConvPadding1006, 1, NumDims.x_saptial_path_convblock1_relu_Relu_outp, NumDims.onnx__Conv_310);
Vars.x_saptial_path_convblock2_conv1_Conv_out = dlconv(Vars.x_saptial_path_convblock1_relu_Relu_outp, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_saptial_path_convblock2_relu_Relu_outp = relu(Vars.x_saptial_path_convblock2_conv1_Conv_out);
NumDims.x_saptial_path_convblock2_relu_Relu_outp = NumDims.x_saptial_path_convblock2_conv1_Conv_out;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_saptial_path_convblock3_conv1_Conv_out] = prepareConvArgs(Vars.onnx__Conv_313, Vars.onnx__Conv_314, Vars.ConvStride1007, Vars.ConvDilationFactor1008, Vars.ConvPadding1009, 1, NumDims.x_saptial_path_convblock2_relu_Relu_outp, NumDims.onnx__Conv_313);
Vars.x_saptial_path_convblock3_conv1_Conv_out = dlconv(Vars.x_saptial_path_convblock2_relu_Relu_outp, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_saptial_path_convblock3_relu_Relu_outp = relu(Vars.x_saptial_path_convblock3_conv1_Conv_out);
NumDims.x_saptial_path_convblock3_relu_Relu_outp = NumDims.x_saptial_path_convblock3_conv1_Conv_out;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_conv1_Conv_output_0] = prepareConvArgs(Vars.onnx__Conv_316, Vars.onnx__Conv_317, Vars.ConvStride1010, Vars.ConvDilationFactor1011, Vars.ConvPadding1012, 1, NumDims.input, NumDims.onnx__Conv_316);
Vars.x_context_path_conv1_Conv_output_0 = dlconv(Vars.input, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_relu_Relu_output_0 = relu(Vars.x_context_path_conv1_Conv_output_0);
NumDims.x_context_path_relu_Relu_output_0 = NumDims.x_context_path_conv1_Conv_output_0;

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.x_context_path_maxpool_MaxPool_output_0] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1013, Vars.MaxPoolStride1014, Vars.MaxPoolPadding1015, NumDims.x_context_path_relu_Relu_output_0);
Vars.x_context_path_maxpool_MaxPool_output_0 = maxpool(Vars.x_context_path_relu_Relu_output_0, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer1_layer1_0_conv1_C_1] = prepareConvArgs(Vars.onnx__Conv_319, Vars.onnx__Conv_320, Vars.ConvStride1016, Vars.ConvDilationFactor1017, Vars.ConvPadding1018, 1, NumDims.x_context_path_maxpool_MaxPool_output_0, NumDims.onnx__Conv_319);
Vars.x_context_path_layer1_layer1_0_conv1_C_1 = dlconv(Vars.x_context_path_maxpool_MaxPool_output_0, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_layer1_layer1_0_relu_Re_1 = relu(Vars.x_context_path_layer1_layer1_0_conv1_C_1);
NumDims.x_context_path_layer1_layer1_0_relu_Re_1 = NumDims.x_context_path_layer1_layer1_0_conv1_C_1;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer1_layer1_0_conv2_C_1] = prepareConvArgs(Vars.onnx__Conv_322, Vars.onnx__Conv_323, Vars.ConvStride1019, Vars.ConvDilationFactor1020, Vars.ConvPadding1021, 1, NumDims.x_context_path_layer1_layer1_0_relu_Re_1, NumDims.onnx__Conv_322);
Vars.x_context_path_layer1_layer1_0_conv2_C_1 = dlconv(Vars.x_context_path_layer1_layer1_0_relu_Re_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.x_context_path_layer1_layer1_0_Add_outpu = Vars.x_context_path_layer1_layer1_0_conv2_C_1 + Vars.x_context_path_maxpool_MaxPool_output_0;
NumDims.x_context_path_layer1_layer1_0_Add_outpu = max(NumDims.x_context_path_layer1_layer1_0_conv2_C_1, NumDims.x_context_path_maxpool_MaxPool_output_0);

% Relu:
Vars.x_context_path_layer1_layer1_0_relu_1__1 = relu(Vars.x_context_path_layer1_layer1_0_Add_outpu);
NumDims.x_context_path_layer1_layer1_0_relu_1__1 = NumDims.x_context_path_layer1_layer1_0_Add_outpu;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer1_layer1_1_conv1_C_1] = prepareConvArgs(Vars.onnx__Conv_325, Vars.onnx__Conv_326, Vars.ConvStride1022, Vars.ConvDilationFactor1023, Vars.ConvPadding1024, 1, NumDims.x_context_path_layer1_layer1_0_relu_1__1, NumDims.onnx__Conv_325);
Vars.x_context_path_layer1_layer1_1_conv1_C_1 = dlconv(Vars.x_context_path_layer1_layer1_0_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_layer1_layer1_1_relu_Re_1 = relu(Vars.x_context_path_layer1_layer1_1_conv1_C_1);
NumDims.x_context_path_layer1_layer1_1_relu_Re_1 = NumDims.x_context_path_layer1_layer1_1_conv1_C_1;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer1_layer1_1_conv2_C_1] = prepareConvArgs(Vars.onnx__Conv_328, Vars.onnx__Conv_329, Vars.ConvStride1025, Vars.ConvDilationFactor1026, Vars.ConvPadding1027, 1, NumDims.x_context_path_layer1_layer1_1_relu_Re_1, NumDims.onnx__Conv_328);
Vars.x_context_path_layer1_layer1_1_conv2_C_1 = dlconv(Vars.x_context_path_layer1_layer1_1_relu_Re_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.x_context_path_layer1_layer1_1_Add_outpu = Vars.x_context_path_layer1_layer1_1_conv2_C_1 + Vars.x_context_path_layer1_layer1_0_relu_1__1;
NumDims.x_context_path_layer1_layer1_1_Add_outpu = max(NumDims.x_context_path_layer1_layer1_1_conv2_C_1, NumDims.x_context_path_layer1_layer1_0_relu_1__1);

% Relu:
Vars.x_context_path_layer1_layer1_1_relu_1__1 = relu(Vars.x_context_path_layer1_layer1_1_Add_outpu);
NumDims.x_context_path_layer1_layer1_1_relu_1__1 = NumDims.x_context_path_layer1_layer1_1_Add_outpu;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer2_layer2_0_conv1_C_1] = prepareConvArgs(Vars.onnx__Conv_331, Vars.onnx__Conv_332, Vars.ConvStride1028, Vars.ConvDilationFactor1029, Vars.ConvPadding1030, 1, NumDims.x_context_path_layer1_layer1_1_relu_1__1, NumDims.onnx__Conv_331);
Vars.x_context_path_layer2_layer2_0_conv1_C_1 = dlconv(Vars.x_context_path_layer1_layer1_1_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_layer2_layer2_0_relu_Re_1 = relu(Vars.x_context_path_layer2_layer2_0_conv1_C_1);
NumDims.x_context_path_layer2_layer2_0_relu_Re_1 = NumDims.x_context_path_layer2_layer2_0_conv1_C_1;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer2_layer2_0_conv2_C_1] = prepareConvArgs(Vars.onnx__Conv_334, Vars.onnx__Conv_335, Vars.ConvStride1031, Vars.ConvDilationFactor1032, Vars.ConvPadding1033, 1, NumDims.x_context_path_layer2_layer2_0_relu_Re_1, NumDims.onnx__Conv_334);
Vars.x_context_path_layer2_layer2_0_conv2_C_1 = dlconv(Vars.x_context_path_layer2_layer2_0_relu_Re_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer2_layer2_0_downsam_1] = prepareConvArgs(Vars.onnx__Conv_337, Vars.onnx__Conv_338, Vars.ConvStride1034, Vars.ConvDilationFactor1035, Vars.ConvPadding1036, 1, NumDims.x_context_path_layer1_layer1_1_relu_1__1, NumDims.onnx__Conv_337);
Vars.x_context_path_layer2_layer2_0_downsam_1 = dlconv(Vars.x_context_path_layer1_layer1_1_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.x_context_path_layer2_layer2_0_Add_outpu = Vars.x_context_path_layer2_layer2_0_conv2_C_1 + Vars.x_context_path_layer2_layer2_0_downsam_1;
NumDims.x_context_path_layer2_layer2_0_Add_outpu = max(NumDims.x_context_path_layer2_layer2_0_conv2_C_1, NumDims.x_context_path_layer2_layer2_0_downsam_1);

% Relu:
Vars.x_context_path_layer2_layer2_0_relu_1__1 = relu(Vars.x_context_path_layer2_layer2_0_Add_outpu);
NumDims.x_context_path_layer2_layer2_0_relu_1__1 = NumDims.x_context_path_layer2_layer2_0_Add_outpu;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer2_layer2_1_conv1_C_1] = prepareConvArgs(Vars.onnx__Conv_340, Vars.onnx__Conv_341, Vars.ConvStride1037, Vars.ConvDilationFactor1038, Vars.ConvPadding1039, 1, NumDims.x_context_path_layer2_layer2_0_relu_1__1, NumDims.onnx__Conv_340);
Vars.x_context_path_layer2_layer2_1_conv1_C_1 = dlconv(Vars.x_context_path_layer2_layer2_0_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_layer2_layer2_1_relu_Re_1 = relu(Vars.x_context_path_layer2_layer2_1_conv1_C_1);
NumDims.x_context_path_layer2_layer2_1_relu_Re_1 = NumDims.x_context_path_layer2_layer2_1_conv1_C_1;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer2_layer2_1_conv2_C_1] = prepareConvArgs(Vars.onnx__Conv_343, Vars.onnx__Conv_344, Vars.ConvStride1040, Vars.ConvDilationFactor1041, Vars.ConvPadding1042, 1, NumDims.x_context_path_layer2_layer2_1_relu_Re_1, NumDims.onnx__Conv_343);
Vars.x_context_path_layer2_layer2_1_conv2_C_1 = dlconv(Vars.x_context_path_layer2_layer2_1_relu_Re_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.x_context_path_layer2_layer2_1_Add_outpu = Vars.x_context_path_layer2_layer2_1_conv2_C_1 + Vars.x_context_path_layer2_layer2_0_relu_1__1;
NumDims.x_context_path_layer2_layer2_1_Add_outpu = max(NumDims.x_context_path_layer2_layer2_1_conv2_C_1, NumDims.x_context_path_layer2_layer2_0_relu_1__1);

% Relu:
Vars.x_context_path_layer2_layer2_1_relu_1__1 = relu(Vars.x_context_path_layer2_layer2_1_Add_outpu);
NumDims.x_context_path_layer2_layer2_1_relu_1__1 = NumDims.x_context_path_layer2_layer2_1_Add_outpu;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer3_layer3_0_conv1_C_1] = prepareConvArgs(Vars.onnx__Conv_346, Vars.onnx__Conv_347, Vars.ConvStride1043, Vars.ConvDilationFactor1044, Vars.ConvPadding1045, 1, NumDims.x_context_path_layer2_layer2_1_relu_1__1, NumDims.onnx__Conv_346);
Vars.x_context_path_layer3_layer3_0_conv1_C_1 = dlconv(Vars.x_context_path_layer2_layer2_1_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_layer3_layer3_0_relu_Re_1 = relu(Vars.x_context_path_layer3_layer3_0_conv1_C_1);
NumDims.x_context_path_layer3_layer3_0_relu_Re_1 = NumDims.x_context_path_layer3_layer3_0_conv1_C_1;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer3_layer3_0_conv2_C_1] = prepareConvArgs(Vars.onnx__Conv_349, Vars.onnx__Conv_350, Vars.ConvStride1046, Vars.ConvDilationFactor1047, Vars.ConvPadding1048, 1, NumDims.x_context_path_layer3_layer3_0_relu_Re_1, NumDims.onnx__Conv_349);
Vars.x_context_path_layer3_layer3_0_conv2_C_1 = dlconv(Vars.x_context_path_layer3_layer3_0_relu_Re_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer3_layer3_0_downsam_1] = prepareConvArgs(Vars.onnx__Conv_352, Vars.onnx__Conv_353, Vars.ConvStride1049, Vars.ConvDilationFactor1050, Vars.ConvPadding1051, 1, NumDims.x_context_path_layer2_layer2_1_relu_1__1, NumDims.onnx__Conv_352);
Vars.x_context_path_layer3_layer3_0_downsam_1 = dlconv(Vars.x_context_path_layer2_layer2_1_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.x_context_path_layer3_layer3_0_Add_outpu = Vars.x_context_path_layer3_layer3_0_conv2_C_1 + Vars.x_context_path_layer3_layer3_0_downsam_1;
NumDims.x_context_path_layer3_layer3_0_Add_outpu = max(NumDims.x_context_path_layer3_layer3_0_conv2_C_1, NumDims.x_context_path_layer3_layer3_0_downsam_1);

% Relu:
Vars.x_context_path_layer3_layer3_0_relu_1__1 = relu(Vars.x_context_path_layer3_layer3_0_Add_outpu);
NumDims.x_context_path_layer3_layer3_0_relu_1__1 = NumDims.x_context_path_layer3_layer3_0_Add_outpu;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer3_layer3_1_conv1_C_1] = prepareConvArgs(Vars.onnx__Conv_355, Vars.onnx__Conv_356, Vars.ConvStride1052, Vars.ConvDilationFactor1053, Vars.ConvPadding1054, 1, NumDims.x_context_path_layer3_layer3_0_relu_1__1, NumDims.onnx__Conv_355);
Vars.x_context_path_layer3_layer3_1_conv1_C_1 = dlconv(Vars.x_context_path_layer3_layer3_0_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_layer3_layer3_1_relu_Re_1 = relu(Vars.x_context_path_layer3_layer3_1_conv1_C_1);
NumDims.x_context_path_layer3_layer3_1_relu_Re_1 = NumDims.x_context_path_layer3_layer3_1_conv1_C_1;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer3_layer3_1_conv2_C_1] = prepareConvArgs(Vars.onnx__Conv_358, Vars.onnx__Conv_359, Vars.ConvStride1055, Vars.ConvDilationFactor1056, Vars.ConvPadding1057, 1, NumDims.x_context_path_layer3_layer3_1_relu_Re_1, NumDims.onnx__Conv_358);
Vars.x_context_path_layer3_layer3_1_conv2_C_1 = dlconv(Vars.x_context_path_layer3_layer3_1_relu_Re_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.x_context_path_layer3_layer3_1_Add_outpu = Vars.x_context_path_layer3_layer3_1_conv2_C_1 + Vars.x_context_path_layer3_layer3_0_relu_1__1;
NumDims.x_context_path_layer3_layer3_1_Add_outpu = max(NumDims.x_context_path_layer3_layer3_1_conv2_C_1, NumDims.x_context_path_layer3_layer3_0_relu_1__1);

% Relu:
Vars.x_context_path_layer3_layer3_1_relu_1__1 = relu(Vars.x_context_path_layer3_layer3_1_Add_outpu);
NumDims.x_context_path_layer3_layer3_1_relu_1__1 = NumDims.x_context_path_layer3_layer3_1_Add_outpu;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer4_layer4_0_conv1_C_1] = prepareConvArgs(Vars.onnx__Conv_361, Vars.onnx__Conv_362, Vars.ConvStride1058, Vars.ConvDilationFactor1059, Vars.ConvPadding1060, 1, NumDims.x_context_path_layer3_layer3_1_relu_1__1, NumDims.onnx__Conv_361);
Vars.x_context_path_layer4_layer4_0_conv1_C_1 = dlconv(Vars.x_context_path_layer3_layer3_1_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_layer4_layer4_0_relu_Re_1 = relu(Vars.x_context_path_layer4_layer4_0_conv1_C_1);
NumDims.x_context_path_layer4_layer4_0_relu_Re_1 = NumDims.x_context_path_layer4_layer4_0_conv1_C_1;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer4_layer4_0_conv2_C_1] = prepareConvArgs(Vars.onnx__Conv_364, Vars.onnx__Conv_365, Vars.ConvStride1061, Vars.ConvDilationFactor1062, Vars.ConvPadding1063, 1, NumDims.x_context_path_layer4_layer4_0_relu_Re_1, NumDims.onnx__Conv_364);
Vars.x_context_path_layer4_layer4_0_conv2_C_1 = dlconv(Vars.x_context_path_layer4_layer4_0_relu_Re_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer4_layer4_0_downsam_1] = prepareConvArgs(Vars.onnx__Conv_367, Vars.onnx__Conv_368, Vars.ConvStride1064, Vars.ConvDilationFactor1065, Vars.ConvPadding1066, 1, NumDims.x_context_path_layer3_layer3_1_relu_1__1, NumDims.onnx__Conv_367);
Vars.x_context_path_layer4_layer4_0_downsam_1 = dlconv(Vars.x_context_path_layer3_layer3_1_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.x_context_path_layer4_layer4_0_Add_outpu = Vars.x_context_path_layer4_layer4_0_conv2_C_1 + Vars.x_context_path_layer4_layer4_0_downsam_1;
NumDims.x_context_path_layer4_layer4_0_Add_outpu = max(NumDims.x_context_path_layer4_layer4_0_conv2_C_1, NumDims.x_context_path_layer4_layer4_0_downsam_1);

% Relu:
Vars.x_context_path_layer4_layer4_0_relu_1__1 = relu(Vars.x_context_path_layer4_layer4_0_Add_outpu);
NumDims.x_context_path_layer4_layer4_0_relu_1__1 = NumDims.x_context_path_layer4_layer4_0_Add_outpu;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer4_layer4_1_conv1_C_1] = prepareConvArgs(Vars.onnx__Conv_370, Vars.onnx__Conv_371, Vars.ConvStride1067, Vars.ConvDilationFactor1068, Vars.ConvPadding1069, 1, NumDims.x_context_path_layer4_layer4_0_relu_1__1, NumDims.onnx__Conv_370);
Vars.x_context_path_layer4_layer4_1_conv1_C_1 = dlconv(Vars.x_context_path_layer4_layer4_0_relu_1__1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_context_path_layer4_layer4_1_relu_Re_1 = relu(Vars.x_context_path_layer4_layer4_1_conv1_C_1);
NumDims.x_context_path_layer4_layer4_1_relu_Re_1 = NumDims.x_context_path_layer4_layer4_1_conv1_C_1;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_context_path_layer4_layer4_1_conv2_C_1] = prepareConvArgs(Vars.onnx__Conv_373, Vars.onnx__Conv_374, Vars.ConvStride1070, Vars.ConvDilationFactor1071, Vars.ConvPadding1072, 1, NumDims.x_context_path_layer4_layer4_1_relu_Re_1, NumDims.onnx__Conv_373);
Vars.x_context_path_layer4_layer4_1_conv2_C_1 = dlconv(Vars.x_context_path_layer4_layer4_1_relu_Re_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Add:
Vars.x_context_path_layer4_layer4_1_Add_outpu = Vars.x_context_path_layer4_layer4_1_conv2_C_1 + Vars.x_context_path_layer4_layer4_0_relu_1__1;
NumDims.x_context_path_layer4_layer4_1_Add_outpu = max(NumDims.x_context_path_layer4_layer4_1_conv2_C_1, NumDims.x_context_path_layer4_layer4_0_relu_1__1);

% Relu:
Vars.x_context_path_layer4_layer4_1_relu_1__1 = relu(Vars.x_context_path_layer4_layer4_1_Add_outpu);
NumDims.x_context_path_layer4_layer4_1_relu_1__1 = NumDims.x_context_path_layer4_layer4_1_Add_outpu;

% ReduceMean:
dims = prepareReduceArgs(Vars.ReduceMeanAxes1073, NumDims.x_context_path_layer4_layer4_1_relu_1__1);
Vars.x_context_path_ReduceMean_output_0 = mean(Vars.x_context_path_layer4_layer4_1_relu_1__1, dims);
NumDims.x_context_path_ReduceMean_output_0 = NumDims.x_context_path_layer4_layer4_1_relu_1__1;

% ReduceMean:
dims = prepareReduceArgs(Vars.ReduceMeanAxes1074, NumDims.x_context_path_ReduceMean_output_0);
Vars.x_context_path_ReduceMean_1_output_0 = mean(Vars.x_context_path_ReduceMean_output_0, dims);
NumDims.x_context_path_ReduceMean_1_output_0 = NumDims.x_context_path_ReduceMean_output_0;

% GlobalAveragePool:
[poolsize, dataFormat, NumDims.x_attention_refinement_module1_avgpool_1] = prepareGlobalAveragePoolArgs(Vars.x_context_path_layer3_layer3_1_relu_1__1, NumDims.x_context_path_layer3_layer3_1_relu_1__1);
Vars.x_attention_refinement_module1_avgpool_1 = avgpool(Vars.x_context_path_layer3_layer3_1_relu_1__1, poolsize, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_attention_refinement_module1_conv_Co_1] = prepareConvArgs(Vars.attention_refinement_module1_conv_weight, Vars.attention_refinement_module1_conv_bias, Vars.ConvStride1075, Vars.ConvDilationFactor1076, Vars.ConvPadding1077, 1, NumDims.x_attention_refinement_module1_avgpool_1, NumDims.attention_refinement_module1_conv_weight);
Vars.x_attention_refinement_module1_conv_Co_1 = dlconv(Vars.x_attention_refinement_module1_avgpool_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Sigmoid:
Vars.x_attention_refinement_module1_sigmoid_1 = sigmoid(Vars.x_attention_refinement_module1_conv_Co_1);
NumDims.x_attention_refinement_module1_sigmoid_1 = NumDims.x_attention_refinement_module1_conv_Co_1;

% Mul:
Vars.x_attention_refinement_module1_Mul_outpu = Vars.x_context_path_layer3_layer3_1_relu_1__1 .* Vars.x_attention_refinement_module1_sigmoid_1;
NumDims.x_attention_refinement_module1_Mul_outpu = max(NumDims.x_context_path_layer3_layer3_1_relu_1__1, NumDims.x_attention_refinement_module1_sigmoid_1);

% GlobalAveragePool:
[poolsize, dataFormat, NumDims.x_attention_refinement_module2_avgpool_1] = prepareGlobalAveragePoolArgs(Vars.x_context_path_layer4_layer4_1_relu_1__1, NumDims.x_context_path_layer4_layer4_1_relu_1__1);
Vars.x_attention_refinement_module2_avgpool_1 = avgpool(Vars.x_context_path_layer4_layer4_1_relu_1__1, poolsize, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_attention_refinement_module2_conv_Co_1] = prepareConvArgs(Vars.attention_refinement_module2_conv_weight, Vars.attention_refinement_module2_conv_bias, Vars.ConvStride1078, Vars.ConvDilationFactor1079, Vars.ConvPadding1080, 1, NumDims.x_attention_refinement_module2_avgpool_1, NumDims.attention_refinement_module2_conv_weight);
Vars.x_attention_refinement_module2_conv_Co_1 = dlconv(Vars.x_attention_refinement_module2_avgpool_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Sigmoid:
Vars.x_attention_refinement_module2_sigmoid_1 = sigmoid(Vars.x_attention_refinement_module2_conv_Co_1);
NumDims.x_attention_refinement_module2_sigmoid_1 = NumDims.x_attention_refinement_module2_conv_Co_1;

% Mul:
Vars.x_attention_refinement_module2_Mul_outpu = Vars.x_context_path_layer4_layer4_1_relu_1__1 .* Vars.x_attention_refinement_module2_sigmoid_1;
NumDims.x_attention_refinement_module2_Mul_outpu = max(NumDims.x_context_path_layer4_layer4_1_relu_1__1, NumDims.x_attention_refinement_module2_sigmoid_1);

% Mul:
Vars.x_Mul_output_0 = Vars.x_attention_refinement_module2_Mul_outpu .* Vars.x_context_path_ReduceMean_1_output_0;
NumDims.x_Mul_output_0 = max(NumDims.x_attention_refinement_module2_Mul_outpu, NumDims.x_context_path_ReduceMean_1_output_0);

% Shape:
[Vars.x_Shape_output_0, NumDims.x_Shape_output_0] = onnxShape(Vars.x_saptial_path_convblock3_relu_Relu_outp, NumDims.x_saptial_path_convblock3_relu_Relu_outp);

% Gather:
[Vars.x_Gather_output_0, NumDims.x_Gather_output_0] = onnxGather(Vars.x_Shape_output_0, Vars.x_Constant_output_0, 0, NumDims.x_Shape_output_0, NumDims.x_Constant_output_0);

% Shape:
[Vars.x_Shape_1_output_0, NumDims.x_Shape_1_output_0] = onnxShape(Vars.x_saptial_path_convblock3_relu_Relu_outp, NumDims.x_saptial_path_convblock3_relu_Relu_outp);

% Gather:
[Vars.x_Gather_1_output_0, NumDims.x_Gather_1_output_0] = onnxGather(Vars.x_Shape_1_output_0, Vars.x_Constant_1_output_0, 0, NumDims.x_Shape_1_output_0, NumDims.x_Constant_1_output_0);

% Unsqueeze:
[shape, NumDims.x_Unsqueeze_output_0] = prepareUnsqueezeArgs(Vars.x_Gather_output_0, Vars.UnsqueezeAxes1081, NumDims.x_Gather_output_0);
Vars.x_Unsqueeze_output_0 = reshape(Vars.x_Gather_output_0, shape);

% Unsqueeze:
[shape, NumDims.x_Unsqueeze_1_output_0] = prepareUnsqueezeArgs(Vars.x_Gather_1_output_0, Vars.UnsqueezeAxes1082, NumDims.x_Gather_1_output_0);
Vars.x_Unsqueeze_1_output_0 = reshape(Vars.x_Gather_1_output_0, shape);

% Concat:
[Vars.x_Concat_output_0, NumDims.x_Concat_output_0] = onnxConcat(0, {Vars.x_Unsqueeze_output_0, Vars.x_Unsqueeze_1_output_0}, [NumDims.x_Unsqueeze_output_0, NumDims.x_Unsqueeze_1_output_0]);

% Unsqueeze:
[shape, NumDims.x_Unsqueeze_2_output_0] = prepareUnsqueezeArgs(Vars.x_Gather_output_0, Vars.UnsqueezeAxes1083, NumDims.x_Gather_output_0);
Vars.x_Unsqueeze_2_output_0 = reshape(Vars.x_Gather_output_0, shape);

% Unsqueeze:
[shape, NumDims.x_Unsqueeze_3_output_0] = prepareUnsqueezeArgs(Vars.x_Gather_1_output_0, Vars.UnsqueezeAxes1084, NumDims.x_Gather_1_output_0);
Vars.x_Unsqueeze_3_output_0 = reshape(Vars.x_Gather_1_output_0, shape);

% Concat:
[Vars.x_Concat_1_output_0, NumDims.x_Concat_1_output_0] = onnxConcat(0, {Vars.x_Unsqueeze_2_output_0, Vars.x_Unsqueeze_3_output_0}, [NumDims.x_Unsqueeze_2_output_0, NumDims.x_Unsqueeze_3_output_0]);

% Shape:
[Vars.x_Shape_2_output_0, NumDims.x_Shape_2_output_0] = onnxShape(Vars.x_attention_refinement_module1_Mul_outpu, NumDims.x_attention_refinement_module1_Mul_outpu);

% Slice:
[Indices, NumDims.x_Slice_output_0] = prepareSliceArgs(Vars.x_Shape_2_output_0, Vars.x_Constant_3_output_0, Vars.x_Constant_4_output_0, Vars.x_Constant_2_output_0, '', NumDims.x_Shape_2_output_0);
Vars.x_Slice_output_0 = subsref(Vars.x_Shape_2_output_0, Indices);

% Cast:
Vars.x_Cast_output_0 = cast(int64(extractdata(Vars.x_Concat_output_0)), 'like', Vars.x_Concat_output_0);
NumDims.x_Cast_output_0 = NumDims.x_Concat_output_0;

% Concat:
[Vars.x_Concat_2_output_0, NumDims.x_Concat_2_output_0] = onnxConcat(0, {Vars.x_Slice_output_0, Vars.x_Cast_output_0}, [NumDims.x_Slice_output_0, NumDims.x_Cast_output_0]);

% Resize:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.x_Resize_output_0] = prepareResize11Args(Vars.x_Constant_5_output_0, Vars.x_Constant_6_output_0, Vars.x_Concat_2_output_0, "half_pixel", "linear", "floor", NumDims.x_attention_refinement_module1_Mul_outpu);
if isempty(DLTScales)
    Vars.x_Resize_output_0 = dlresize(Vars.x_attention_refinement_module1_Mul_outpu, 'OutputSize', DLTSizes, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);
else
    Vars.x_Resize_output_0 = dlresize(Vars.x_attention_refinement_module1_Mul_outpu, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);
end

% Shape:
[Vars.x_Shape_3_output_0, NumDims.x_Shape_3_output_0] = onnxShape(Vars.x_Mul_output_0, NumDims.x_Mul_output_0);

% Slice:
[Indices, NumDims.x_Slice_1_output_0] = prepareSliceArgs(Vars.x_Shape_3_output_0, Vars.x_Constant_8_output_0, Vars.x_Constant_9_output_0, Vars.x_Constant_7_output_0, '', NumDims.x_Shape_3_output_0);
Vars.x_Slice_1_output_0 = subsref(Vars.x_Shape_3_output_0, Indices);

% Cast:
Vars.x_Cast_1_output_0 = cast(int64(extractdata(Vars.x_Concat_1_output_0)), 'like', Vars.x_Concat_1_output_0);
NumDims.x_Cast_1_output_0 = NumDims.x_Concat_1_output_0;

% Concat:
[Vars.x_Concat_3_output_0, NumDims.x_Concat_3_output_0] = onnxConcat(0, {Vars.x_Slice_1_output_0, Vars.x_Cast_1_output_0}, [NumDims.x_Slice_1_output_0, NumDims.x_Cast_1_output_0]);

% Resize:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.x_Resize_1_output_0] = prepareResize11Args(Vars.x_Constant_10_output_0, Vars.x_Constant_11_output_0, Vars.x_Concat_3_output_0, "half_pixel", "linear", "floor", NumDims.x_Mul_output_0);
if isempty(DLTScales)
    Vars.x_Resize_1_output_0 = dlresize(Vars.x_Mul_output_0, 'OutputSize', DLTSizes, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);
else
    Vars.x_Resize_1_output_0 = dlresize(Vars.x_Mul_output_0, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);
end

% Concat:
[Vars.x_feature_fusion_module_Concat_output_0, NumDims.x_feature_fusion_module_Concat_output_0] = onnxConcat(1, {Vars.x_saptial_path_convblock3_relu_Relu_outp, Vars.x_Resize_output_0, Vars.x_Resize_1_output_0}, [NumDims.x_saptial_path_convblock3_relu_Relu_outp, NumDims.x_Resize_output_0, NumDims.x_Resize_1_output_0]);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_feature_fusion_module_convblock_conv_1] = prepareConvArgs(Vars.onnx__Conv_376, Vars.onnx__Conv_377, Vars.ConvStride1085, Vars.ConvDilationFactor1086, Vars.ConvPadding1087, 1, NumDims.x_feature_fusion_module_Concat_output_0, NumDims.onnx__Conv_376);
Vars.x_feature_fusion_module_convblock_conv_1 = dlconv(Vars.x_feature_fusion_module_Concat_output_0, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_feature_fusion_module_convblock_relu_1 = relu(Vars.x_feature_fusion_module_convblock_conv_1);
NumDims.x_feature_fusion_module_convblock_relu_1 = NumDims.x_feature_fusion_module_convblock_conv_1;

% GlobalAveragePool:
[poolsize, dataFormat, NumDims.x_feature_fusion_module_avgpool_Global_1] = prepareGlobalAveragePoolArgs(Vars.x_feature_fusion_module_convblock_relu_1, NumDims.x_feature_fusion_module_convblock_relu_1);
Vars.x_feature_fusion_module_avgpool_Global_1 = avgpool(Vars.x_feature_fusion_module_convblock_relu_1, poolsize, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_feature_fusion_module_conv1_Conv_outpu] = prepareConvArgs(Vars.feature_fusion_module_conv1_weight, Vars.feature_fusion_module_conv1_bias, Vars.ConvStride1088, Vars.ConvDilationFactor1089, Vars.ConvPadding1090, 1, NumDims.x_feature_fusion_module_avgpool_Global_1, NumDims.feature_fusion_module_conv1_weight);
Vars.x_feature_fusion_module_conv1_Conv_outpu = dlconv(Vars.x_feature_fusion_module_avgpool_Global_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x_feature_fusion_module_relu_Relu_output = relu(Vars.x_feature_fusion_module_conv1_Conv_outpu);
NumDims.x_feature_fusion_module_relu_Relu_output = NumDims.x_feature_fusion_module_conv1_Conv_outpu;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x_feature_fusion_module_conv2_Conv_outpu] = prepareConvArgs(Vars.feature_fusion_module_conv2_weight, Vars.feature_fusion_module_conv2_bias, Vars.ConvStride1091, Vars.ConvDilationFactor1092, Vars.ConvPadding1093, 1, NumDims.x_feature_fusion_module_relu_Relu_output, NumDims.feature_fusion_module_conv2_weight);
Vars.x_feature_fusion_module_conv2_Conv_outpu = dlconv(Vars.x_feature_fusion_module_relu_Relu_output, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Sigmoid:
Vars.x_feature_fusion_module_sigmoid_Sigmoid_ = sigmoid(Vars.x_feature_fusion_module_conv2_Conv_outpu);
NumDims.x_feature_fusion_module_sigmoid_Sigmoid_ = NumDims.x_feature_fusion_module_conv2_Conv_outpu;

% Mul:
Vars.x_feature_fusion_module_Mul_output_0 = Vars.x_feature_fusion_module_convblock_relu_1 .* Vars.x_feature_fusion_module_sigmoid_Sigmoid_;
NumDims.x_feature_fusion_module_Mul_output_0 = max(NumDims.x_feature_fusion_module_convblock_relu_1, NumDims.x_feature_fusion_module_sigmoid_Sigmoid_);

% Add:
Vars.x_feature_fusion_module_Add_output_0 = Vars.x_feature_fusion_module_Mul_output_0 + Vars.x_feature_fusion_module_convblock_relu_1;
NumDims.x_feature_fusion_module_Add_output_0 = max(NumDims.x_feature_fusion_module_Mul_output_0, NumDims.x_feature_fusion_module_convblock_relu_1);

% Resize:
[DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, NumDims.x_Resize_2_output_0] = prepareResize11Args(Vars.x_Constant_13_output_0, Vars.x_Constant_12_output_0, dlarray([]), "half_pixel", "linear", "floor", NumDims.x_feature_fusion_module_Add_output_0);
if isempty(DLTScales)
    Vars.x_Resize_2_output_0 = dlresize(Vars.x_feature_fusion_module_Add_output_0, 'OutputSize', DLTSizes, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);
else
    Vars.x_Resize_2_output_0 = dlresize(Vars.x_feature_fusion_module_Add_output_0, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', GeometricTransformMode, 'NearestRoundingMode', NearestRoundingMode);
end

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.output] = prepareConvArgs(Vars.conv_weight, Vars.conv_bias, Vars.ConvStride1094, Vars.ConvDilationFactor1095, Vars.ConvPadding1096, 1, NumDims.x_Resize_2_output_0, NumDims.conv_weight);
Vars.output = dlconv(Vars.x_Resize_2_output_0, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Set graph output arguments from Vars and NumDims:
output = Vars.output;
outputNumDims1098 = NumDims.output;
% Set output state from Vars:
state = updateStruct(state, Vars);
end

function [inputDataPerms, outputDataPerms, Training] = parseInputs(input, numDataOutputs, params, varargin)
% Function to validate inputs to BiSeNetFunc:
p = inputParser;
isValidArrayInput = @(x)isnumeric(x) || isstring(x);
isValidONNXParameters = @(x)isa(x, 'ONNXParameters');
addRequired(p, 'input', isValidArrayInput);
addRequired(p, 'params', isValidONNXParameters);
addParameter(p, 'InputDataPermutation', 'auto');
addParameter(p, 'OutputDataPermutation', 'auto');
addParameter(p, 'Training', false);
parse(p, input, params, varargin{:});
inputDataPerms = p.Results.InputDataPermutation;
outputDataPerms = p.Results.OutputDataPermutation;
Training = p.Results.Training;
if isnumeric(inputDataPerms)
    inputDataPerms = {inputDataPerms};
end
if isstring(inputDataPerms) && isscalar(inputDataPerms) || ischar(inputDataPerms)
    inputDataPerms = repmat({inputDataPerms},1,1);
end
if isnumeric(outputDataPerms)
    outputDataPerms = {outputDataPerms};
end
if isstring(outputDataPerms) && isscalar(outputDataPerms) || ischar(outputDataPerms)
    outputDataPerms = repmat({outputDataPerms},1,numDataOutputs);
end
end

function [input, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(input, params, varargin)
% Parse input arguments
[inputDataPerms, outputDataPerms, Training] = parseInputs(input, 1, params, varargin{:});
anyDlarrayInputs = any(cellfun(@(x)isa(x, 'dlarray'), {input}));
% Make the input variables into unlabelled dlarrays:
input = makeUnlabeledDlarray(input);
% Permute inputs if requested:
input = permuteInputVar(input, inputDataPerms{1}, 4);
% Check input size(s):
checkInputSize(size(input), {'batch_size' 3 720 960}, "input");
end

function [output] = postprocessOutput(output, outputDataPerms, anyDlarrayInputs, Training, varargin)
% Set output type:
if ~anyDlarrayInputs && ~Training
    if isdlarray(output)
        output = extractdata(output);
    end
end
% Permute outputs if requested:
output = permuteOutputVar(output, outputDataPerms{1}, 4);
end


%% dlarray functions implementing ONNX operators:

function [Y, numDimsY] = onnxConcat(ONNXAxis, XCell, numDimsXArray)
% Concatentation that treats all empties the same. Necessary because
% dlarray.cat does not allow, for example, cat(1, 1x1, 1x0) because the
% second dimension sizes do not match.
numDimsY = numDimsXArray(1);
XCell(cellfun(@isempty, XCell)) = [];
if isempty(XCell)
    Y = dlarray([]);
else
    if ONNXAxis<0
        ONNXAxis = ONNXAxis + numDimsY;
    end
    DLTAxis = numDimsY - ONNXAxis;
    Y = cat(DLTAxis, XCell{:});
end
end

function [Y, numDimsY] = onnxGather(X, ONNXIdx, ONNXAxis, numDimsX, numDimsIdx)
% Function implementing the ONNX Gather operator

% In ONNX, 'Gather' first indexes into dimension ONNXAxis of data, using
% the contents of ONNXIdx as the indices. Then, it reshapes the ONNXAxis
% into the shape of ONNXIdx.
%   Example 1:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6 7], and axis=1.
% The result has shape [2 6 7 4 5].
%   Example 2:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6], and axis=1.
% The result has shape [2 6 4 5].
%   Example 3:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [] (a scalar), and axis=1.
% The result has shape [2 4 5].
%
% Since we're using reverse indexing relative to ONNX, in this function
% data and ONNXIdx both have reversed dimension ordering.
numDimsY = numDimsIdx + (numDimsX - 1);
if isempty(X)
    Y = X;
    return;
end
% (1) First, do the subsref part of Gather
if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsX;                                 % Axis can be negative. Convert it to its positive equivalent.
end
dltAxis = numDimsX - ONNXAxis;                                      % Convert axis to DLT. ONNXAxis is origin 0 and we index from the end
ONNXIdx(ONNXIdx<0) = ONNXIdx(ONNXIdx<0) + size(X, dltAxis);         % ONNXIdx can have negative components. Make them positive.
dltIdx  = extractdata(ONNXIdx) + 1;                                 % ONNXIdx is origin-0 in ONNX, so add 1 to get dltIdx
% Use subsref to index into data
Indices.subs = repmat({':'}, 1, numDimsX);
Indices.subs{dltAxis} = dltIdx(:);                                  % Index as a column to ensure the output is 1-D in the indexed dimension (for now).
Indices.type = '()';
Y = subsref(X, Indices);
% (2) Now do the reshaping part of Gather
shape = size(Y, 1:numDimsX);
if numDimsIdx == 0
    % Delete the indexed dimension
    shape(dltAxis) = [];
elseif numDimsIdx > 1
    % Reshape the indexed dimension into the shape of ONNXIdx
    shape = [shape(1:dltAxis-1) size(ONNXIdx, 1:numDimsIdx) shape(dltAxis+1:end)];
end
% Extend the shape to 2D so it's valid MATLAB
if numel(shape) < 2
    shape = [shape ones(1,2-numel(shape))];
end
Y = reshape(Y, shape);
end

function [Y, numDimsY] = onnxShape(X, numDimsX)
% Implements the ONNX Shape operator
% Return the reverse ONNX shape as a 1D column vector
switch numDimsX
    case 0
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(1);
        end
    case 1
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(size(X,1));
        end
    otherwise
        Y = dlarray(fliplr(size(X, 1:numDimsX))');
end
numDimsY = 1;
end

function [weights, bias, stride, dilationFactor, padding, dataFormat, numDimsY] = prepareConvArgs(...
    weights, bias, stride, dilationFactor, padding, numWtGroups, numDimsX, numDimsW)
% Prepares arguments for implementing the ONNX Conv operator

% Weights: The ONNX weight dim is Fcxyz..., where c=C/G, G is numGroups,
% and xyz... are spatial dimensions. DLT "weights" here is the flip of
% that, or ...zyxcF. dlconv requires ...zyxcfG, where f=F/G. So reshape to
% split the last dimension.
sizeW    = size(weights, 1:numDimsW);
F        = sizeW(end);
newWSize = [sizeW(1:numDimsW-1), F/numWtGroups, numWtGroups];
weights  = reshape(weights, newWSize);
% bias
if isempty(bias)
    bias = 0;
end
bias = dlarray(bias(:),'CU');
% Derive missing default attributes from weight tensor
numSpatialDims = numDimsW-2;
if isempty(padding)
    padding = zeros(1, 2*numSpatialDims);
end
if isempty(stride)
    stride = ones(1,numSpatialDims);
end
if isempty(dilationFactor)
    dilationFactor = ones(1,numSpatialDims);
end
% Make the attributes non-dlarrays:
if isa(stride, 'dlarray')
    stride = extractdata(stride);
end
if isa(dilationFactor, 'dlarray')
    dilationFactor = extractdata(dilationFactor);
end
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
% Make the attributes double row vectors, and flip their dimension ordering
% to reverse-onnx:
stride = fliplr(double(stride(:)'));
dilationFactor = fliplr(double(dilationFactor(:)'));
if isnumeric(padding)       % padding can be "same"
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
% Set dataformat and numdims
dataFormat = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY = numDimsX;
end

function [poolsize, dataFormat, numDimsX] = prepareGlobalAveragePoolArgs(X, numDimsX)
% Prepares arguments for implementing the ONNX GlobalAveragePool operator
poolsize    = size(X, 1:numDimsX-2);                   % The spatial dimensions
dataFormat  = [repmat('S', [1 numDimsX-2]) 'CB'];
end

function [poolsize, stride, padding, dataFormat, numDimsY, numDimsIndices] = prepareMaxPool8Args(poolsize, stride, padding, numDimsX)
% Prepares arguments for implementing the ONNX MaxPool-8 operator
poolsize    = fliplr(extractdata(poolsize(:)'));
stride      = fliplr(extractdata(stride(:)'));
% padding
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
if isnumeric(padding)
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
dataFormat  = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY    = numDimsX;
numDimsIndices = numDimsX;                      % New in opset 8
end

function dims = prepareReduceArgs(ONNXAxes, numDimsX)
% Prepares arguments for implementing the ONNX Reduce operator
if isempty(ONNXAxes)
    ONNXAxes = 0:numDimsX-1;   % All axes
end
ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsX;
dims = numDimsX - ONNXAxes;
end

function [DLTScales, DLTSizes, dataFormat, Method, GeometricTransformMode, NearestRoundingMode, numDimsY] = prepareResize11Args(...
    ONNXRoi, ONNXScales, ONNXSizes, coordinate_transformation_mode, mode, nearest_mode, numDimsX)
% Prepares arguments for implementing the ONNX Resize-11 operator

% ONNXScales and ONNXSizes are in ONNX dimension ordering. ONNXRoi is
% ignored because it only takes effect when coordinate_transformation_mode
% is "tf_crop_and_resize", which is not supported.
DLTScales = flip(extractdata(ONNXScales(:)'));
DLTSizes = flip(extractdata(ONNXSizes(:)'));
switch coordinate_transformation_mode
    case "half_pixel"
        GeometricTransformMode = "half-pixel";
    case "asymmetric"
        GeometricTransformMode = "asymmetric";
    otherwise
        assert(false);
end
switch mode
    case "nearest"
        Method = "nearest";
    case "linear"
        Method = "linear";
    otherwise
        assert(false);
end
switch nearest_mode
    case "floor"
        NearestRoundingMode = "floor";
    otherwise
        NearestRoundingMode = "round";
end
dataFormat = repmat('S', [1 numDimsX]);
numDimsY = numDimsX;
end

function [S, numDimsY] = prepareSliceArgs(X, Starts, Ends, Axes, Steps, numDimsX)
% Prepares arguments for implementing the ONNX Slice operator

% Starts, Ends and Axes are all origin 0. Axes refer to the ONNX dimension
% ordering, but X uses the reverse, DLT ordering. Starts, Ends, Axes, and
% Steps correspond positionally. Axes and Steps may be omitted, with
% defaults described in the ONNX spec.

% Set default Axes and Steps if not supplied
if isempty(Axes)
    Axes = 0:numDimsX-1;   % All axes
end
Axes(Axes<0) = Axes(Axes<0) + numDimsX; % Handle negative Axes.
if isempty(Steps)
    Steps = ones(1, numel(Starts));
end
% Init all dims to :
S.subs = repmat({':'}, 1, numDimsX);
S.type = '()';
% Set Starts and Ends for each axis
for i = 1:numel(Axes)
    DLTDim = numDimsX - Axes(i);                                               % The DLT dim is the reverse of the ONNX dim.
    % "If a negative value is passed for any of the start or end indices,
    % it represents number of elements before the end of that dimension."
    if Starts(i) < 0
        Starts(i) = size(X,DLTDim) + Starts(i);
    end
    if Ends(i) < 0
        Ends(i) = max(-1, size(X,DLTDim) + Ends(i));                        % The -1 case is when we're slicing backward and want to include 0.
    end
    % "If the value passed to start or end is larger than the n (the number
    % of elements in this dimension), it represents n."
    if Starts(i) > size(X,DLTDim)
        Starts(i) = size(X,DLTDim);
    end
    if Ends(i) > size(X,DLTDim)
        Ends(i) = size(X,DLTDim);
    end
    if Steps(i) > 0
        S.subs{DLTDim} = 1 + (Starts(i) : Steps(i) : Ends(i)-1);            % 1 + (Origin 0 indexing with end index excluded)
    else
        S.subs{DLTDim} = 1 + (Starts(i) : Steps(i) : Ends(i)+1);            % 1 + (Origin 0 indexing with end index excluded)
    end
end
numDimsY = numDimsX;
end

function [newShape, numDimsY] = prepareUnsqueezeArgs(X, ONNXAxes, numDimsX)
% Prepares arguments for implementing the ONNX Unsqueeze operator
numDimsY = numDimsX + numel(ONNXAxes);
ONNXAxes = extractdata(ONNXAxes);
ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsY;
ONNXAxes = sort(ONNXAxes);                                              % increasing order
if numDimsY == 1
    newShape = size(X);
else
    DLTAxes  = flip(numDimsY - ONNXAxes);                                  % increasing order
    newShape = ones(1, numDimsY);
    posToSet = setdiff(1:numDimsY, DLTAxes, 'stable');
    newShape(posToSet) = size(X, 1:numel(posToSet));
end
end

%% Utility functions:

function s = appendStructs(varargin)
% s = appendStructs(s1, s2,...). Assign all fields in s1, s2,... into s.
if isempty(varargin)
    s = struct;
else
    s = varargin{1};
    for i = 2:numel(varargin)
        fromstr = varargin{i};
        fs = fieldnames(fromstr);
        for j = 1:numel(fs)
            s.(fs{j}) = fromstr.(fs{j});
        end
    end
end
end

function checkInputSize(inputShape, expectedShape, inputName)

if numel(expectedShape)==0
    % The input is a scalar
    if ~isequal(inputShape, [1 1])
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, "[1,1]", inputSizeStr));
    end
elseif numel(expectedShape)==1
    % The input is a vector
    if ~shapeIsColumnVector(inputShape) || ~iSizesMatch({inputShape(1)}, expectedShape)
        expectedShape{2} = 1;
        expectedSizeStr = makeSizeString(expectedShape);
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
else
    % The input has 2 dimensions or more
    
    % The input dimensions have been reversed; flip them back to compare to the
    % expected ONNX shape.
    inputShape = fliplr(inputShape);
    
    % If the expected shape has fewer dims than the input shape, error.
    if numel(expectedShape) < numel(inputShape)
        expectedSizeStr = strjoin(["[", strjoin(string(expectedShape), ","), "]"], "");
        error(message('nnet_cnn_onnx:onnx:InputHasGreaterNDims', inputName, expectedSizeStr));
    end
    
    % Prepad the input shape with trailing ones up to the number of elements in
    % expectedShape
    inputShape = num2cell([ones(1, numel(expectedShape) - length(inputShape)) inputShape]);
    
    % Find the number of variable size dimensions in the expected shape
    numVariableInputs = sum(cellfun(@(x) isa(x, 'char') || isa(x, 'string'), expectedShape));
    
    % Find the number of input dimensions that are not in the expected shape
    % and cannot be represented by a variable dimension
    nonMatchingInputDims = setdiff(string(inputShape), string(expectedShape));
    numNonMatchingInputDims  = numel(nonMatchingInputDims) - numVariableInputs;
    
    expectedSizeStr = makeSizeString(expectedShape);
    inputSizeStr = makeSizeString(inputShape);
    if numNonMatchingInputDims == 0 && ~iSizesMatch(inputShape, expectedShape)
        % The actual and expected input dimensions match, but in
        % a different order. The input needs to be permuted.
        error(message('nnet_cnn_onnx:onnx:InputNeedsPermute',inputName, expectedSizeStr, inputSizeStr));
    elseif numNonMatchingInputDims > 0
        % The actual and expected input sizes do not match.
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
end
end

function doesMatch = iSizesMatch(inputShape, expectedShape)
% Check whether the input and expected shapes match, in order.
% Size elements match if (1) the elements are equal, or (2) the expected
% size element is a variable (represented by a character vector or string)
doesMatch = true;
for i=1:numel(inputShape)
    if ~(isequal(inputShape{i},expectedShape{i}) || ischar(expectedShape{i}) || isstring(expectedShape{i}))
        doesMatch = false;
        return
    end
end
end

function sizeStr = makeSizeString(shape)
sizeStr = strjoin(["[", strjoin(string(shape), ","), "]"], "");
end

function isVec = shapeIsColumnVector(shape)
if numel(shape) == 2 && shape(2) == 1
    isVec = true;
else
    isVec = false;
end
end
function X = makeUnlabeledDlarray(X)
% Make numeric X into an unlabelled dlarray
if isa(X, 'dlarray')
    X = stripdims(X);
elseif isnumeric(X)
    if isinteger(X)
        % Make ints double so they can combine with anything without
        % reducing precision
        X = double(X);
    end
    X = dlarray(X);
end
end

function [Vars, NumDims] = packageVariables(params, inputNames, inputValues, inputNumDims)
% inputNames, inputValues are cell arrays. inputRanks is a numeric vector.
Vars = appendStructs(params.Learnables, params.Nonlearnables, params.State);
NumDims = params.NumDimensions;
% Add graph inputs
for i = 1:numel(inputNames)
    Vars.(inputNames{i}) = inputValues{i};
    NumDims.(inputNames{i}) = inputNumDims(i);
end
end

function X = permuteInputVar(X, userDataPerm, onnxNDims)
% Returns reverse-ONNX ordering
if onnxNDims == 0
    return;
elseif onnxNDims == 1 && isvector(X)
    X = X(:);
    return;
elseif isnumeric(userDataPerm)
    % Permute into reverse ONNX ordering
    if numel(userDataPerm) ~= onnxNDims
        error(message('nnet_cnn_onnx:onnx:InputPermutationSize', numel(userDataPerm), onnxNDims));
    end
    perm = fliplr(userDataPerm);
elseif isequal(userDataPerm, 'auto') && onnxNDims == 4
    % Permute MATLAB HWCN to reverse onnx (WHCN)
    perm = [2 1 3 4];
elseif isequal(userDataPerm, 'as-is')
    % Do not permute the input
    perm = 1:ndims(X);
else
    % userDataPerm is either 'none' or 'auto' with no default, which means
    % it's already in onnx ordering, so just make it reverse onnx
    perm = max(2,onnxNDims):-1:1;
end
X = permute(X, perm);
end

function Y = permuteOutputVar(Y, userDataPerm, onnxNDims)
switch onnxNDims
    case 0
        perm = [];
    case 1
        if isnumeric(userDataPerm)
            % Use the user's permutation because Y is a column vector which
            % already matches ONNX.
            perm = userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            % Treat the 1D onnx vector as a 2D column and transpose it
            perm = [2 1];
        else
            % userDataPerm is 'none'. Leave Y alone because it already
            % matches onnx.
            perm = [];
        end
    otherwise
        % ndims >= 2
        if isnumeric(userDataPerm)
            % Use the inverse of the user's permutation. This is not just the
            % flip of the permutation vector.
            perm = onnxNDims + 1 - userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            if onnxNDims == 2
                % Permute reverse ONNX CN to DLT CN (do nothing)
                perm = [];
            elseif onnxNDims == 4
                % Permute reverse onnx (WHCN) to MATLAB HWCN
                perm = [2 1 3 4];
            else
                % User wants the output in ONNX ordering, so just reverse it from
                % reverse onnx
                perm = onnxNDims:-1:1;
            end
        elseif isequal(userDataPerm, 'as-is')
            % Do not permute the input
            perm = 1:ndims(Y);
        else
            % userDataPerm is 'none', so just make it reverse onnx
            perm = onnxNDims:-1:1;
        end
end
if ~isempty(perm)
    Y = permute(Y, perm);
end
end

function s = updateStruct(s, t)
% Set all existing fields in s from fields in t, ignoring extra fields in t.
for name = transpose(fieldnames(s))
    s.(name{1}) = t.(name{1});
end
end
